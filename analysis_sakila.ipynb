{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "71f4839e",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Install necessary libraries\n",
    "!pip install langchain langchain-community transformers accelerate tiktoken\n",
    "\n",
    "# Step 2: Clone the SakilaProject repository\n",
    "!git clone https://github.com/janjakovacevic/SakilaProject\n",
    "\n",
    "# Step 3: Check and read Java files from the cloned project\n",
    "import os\n",
    "import json\n",
    "\n",
    "# Update the path to reflect where the GitHub repo is cloned\n",
    "project_path = '/content/SakilaProject'\n",
    "print(\"Files detected:\")\n",
    "\n",
    "# Check and read all .java files from the project\n",
    "all_code = \"\"\n",
    "for root, dirs, files in os.walk(project_path):\n",
    "    for file in files:\n",
    "        if file.endswith(\".java\"):  # Looking for Java files instead of Python\n",
    "            print(f\"Found Java file: {os.path.join(root, file)}\")\n",
    "            with open(os.path.join(root, file), 'r', encoding='utf-8') as f:\n",
    "                content = f.read()\n",
    "                if content.strip() != \"\":\n",
    "                    all_code += f\"\\n\\n# File: {file}\\n\\n\" + content\n",
    "\n",
    "print(f\"\\nTotal characters of Java code read: {len(all_code)}\")\n",
    "\n",
    "# Step 4: Chunk the code into smaller pieces for processing\n",
    "chunks = [all_code[i:i+1000] for i in range(0, len(all_code), 1000)]\n",
    "print(f\"Total number of chunks: {len(chunks)}\")\n",
    "\n",
    "# Step 5: Load the HuggingFace model and LangChain pipeline\n",
    "from transformers import pipeline\n",
    "from langchain_community.llms import HuggingFacePipeline\n",
    "\n",
    "pipe = pipeline(\"text2text-generation\", model=\"google/flan-t5-small\", max_length=512)\n",
    "llm = HuggingFacePipeline(pipeline=pipe)\n",
    "\n",
    "# Step 6: Analyze each chunk using the model\n",
    "results = []\n",
    "\n",
    "for idx, chunk in enumerate(chunks):\n",
    "    print(f\"Analyzing chunk {idx+1}/{len(chunks)}...\")\n",
    "    prompt = f\"\"\"\n",
    "    Analyze this Java code:\n",
    "    - High-level purpose\n",
    "    - Key methods\n",
    "    - Complexity\n",
    "\n",
    "    Code:\n",
    "    {chunk}\n",
    "    \"\"\"\n",
    "\n",
    "    try:\n",
    "        response = llm.invoke(prompt)\n",
    "        results.append({\n",
    "            \"chunk_index\": idx,\n",
    "            \"analysis\": response\n",
    "        })\n",
    "    except Exception as e:\n",
    "        print(f\"Error analyzing chunk {idx}: {e}\")\n",
    "\n",
    "# Step 7: Save the structured output to a JSON file\n",
    "if results:\n",
    "    with open('/content/sakila_analysis.json', 'w') as f:\n",
    "        json.dump(results, f, indent=4)\n",
    "    print(\"✅ JSON file saved at /content/sakila_analysis.json\")\n",
    "else:\n",
    "    print(\"❌ No results generated! Something went wrong.\")\n",
    "\n",
    "# Step 8: Optionally, display the JSON content in Colab\n",
    "import pprint\n",
    "with open('/content/sakila_analysis.json', 'r') as f:\n",
    "    data = json.load(f)\n",
    "\n",
    "# Pretty print the JSON output\n",
    "pprint.pprint(data)\n"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
